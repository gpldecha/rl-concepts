{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on the code from [Juliani's tutorial](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724)\n",
    "This is an implemenation of vanila REINFORCE and only uses the reward function, no value function is used. See [Sutton's 2nd edition book Chapter 13.3](http://incompleteideas.net/book/bookdraft2017nov5.pdf) for a mathematical reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n",
      "/usr/local/lib/python2.7/dist-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add*gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes regading the discount_rewards code\n",
    "\n",
    "...\n",
    "\n",
    "discounted_r[-3] = $R_{T-2} + \\gamma \\, R_{T-1} + \\gamma^2 \\, R_T$\n",
    "\n",
    "discounted_r[-2] = $R_{T-1} + \\gamma \\, R_T$\n",
    "\n",
    "discounted_r[-1] = $R_T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "  \n",
    "    def __init__(self, lr, s_size, a_size, h_size):\n",
    "        # These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "        self.state_in = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "        hidden = slim.fully_connected(self.state_in, h_size, biases_initializer=None, activation_fn=tf.nn.relu)\n",
    "        self.probability = slim.fully_connected(hidden, a_size, activation_fn=tf.nn.softmax, biases_initializer=None)\n",
    "        self.chosen_action = tf.argmax(self.probability, 1)\n",
    "\n",
    "        # The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "        # to compute the loss, and use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "\n",
    "        self.indexes = tf.range(0, tf.shape(self.probability)[0]) * tf.shape(self.probability)[1] + self.action_holder\n",
    "        # gets the probability which were associated with each action\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.probability, [-1]), self.indexes)\n",
    "\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs) * self.reward_holder)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx, var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32, name=str(idx) + '_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "\n",
    "        self.gradients = tf.gradients(self.loss, tvars)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, tvars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes regading the agent code\n",
    "\n",
    "The dimension of the variable:\n",
    "\n",
    "```python\n",
    "    self.probability \n",
    "```\n",
    "\n",
    "will be $(batch \\times 2)$ as their are two actions the batch size is the length of the episode. The loss function needs the probabilities associated with the actions taken. The lines below achieve this\n",
    "\n",
    "    \n",
    "```python\n",
    "    self.indexes = tf.range(0, tf.shape(self.probability)[0]) * tf.shape(self.probability)[1] + self.action_holder\n",
    "    # gets the probability which were associated with each action\n",
    "    self.responsible_outputs = tf.gather(tf.reshape(self.probability, [-1]), self.indexes)\n",
    "```\n",
    "\n",
    "where self.responsible_outputs is a vector of probabilities associated with each action. \n",
    "\n",
    "This line computes the partial derivate of the agent's model parameters with respect to the cost function.\n",
    "\n",
    "```python\n",
    "    self.gradients = tf.gradients(self.loss, tvars)\n",
    "```\n",
    "\n",
    "In our case we have two fully connected layers: $$\\left[\\frac{\\partial\\, J}{\\partial W_1},\\;\\frac{\\partial\\, J}{\\partial W_2}\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  # Clear the Tensorflow graph.\n",
    "\n",
    "# setup the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "agent = Agent(lr=1e-2, s_size=4, a_size=2, h_size=8)  # Load the agent.\n",
    "gamma = 0.99\n",
    "total_episodes = 5000  # Set total number of episodes to train agent on.\n",
    "max_ep = 999\n",
    "update_frequency = 5 # how many MC samples of the gradient we gather before we make an update to the policy.\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.0\n",
      "32.02\n",
      "38.26\n",
      "49.28\n",
      "54.26\n",
      "71.62\n",
      "89.92\n",
      "114.57\n",
      "152.14\n",
      "169.07\n",
      "177.28\n",
      "175.89\n",
      "187.6\n",
      "190.82\n",
      "191.55\n",
      "180.41\n",
      "180.13\n",
      "191.55\n",
      "193.19\n",
      "195.05\n",
      "194.53\n",
      "195.92\n",
      "186.88\n",
      "194.7\n",
      "196.36\n",
      "199.82\n",
      "196.15\n",
      "190.0\n",
      "188.02\n",
      "189.79\n",
      "179.66\n",
      "198.74\n",
      "197.65\n",
      "199.17\n",
      "199.77\n",
      "200.0\n",
      "199.28\n",
      "200.0\n",
      "194.08\n",
      "197.92\n",
      "198.67\n",
      "194.64\n",
      "197.26\n",
      "185.44\n",
      "195.81\n",
      "200.0\n",
      "199.66\n",
      "200.0\n",
      "200.0\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    total_reward = []\n",
    "    total_lenght = []\n",
    "\n",
    "    # Setting gradients of the policy to ZERO dc_dw1, dc_dw2\n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "\n",
    "    while i < total_episodes:\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_ep):\n",
    "            # Probabilistically pick an action given our network outputs.\n",
    "            a_dist = sess.run(agent.probability, feed_dict={agent.state_in: [s]})\n",
    "            a = np.random.choice(a_dist[0], p=a_dist[0])\n",
    "            a = np.argmax(a_dist == a)\n",
    "\n",
    "            s1, r, d, _ = env.step(a)  # Get our reward for taking an action given a bandit.\n",
    "            ep_history.append([s, a, r, s1])\n",
    "            s = s1\n",
    "            running_reward += r\n",
    "            if d == True:\n",
    "                # Update the network.\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:, 2] = discount_rewards(ep_history[:, 2], gamma)\n",
    "                feed_dict = {agent.reward_holder: ep_history[:, 2],\n",
    "                             agent.action_holder: ep_history[:, 1], agent.state_in: np.vstack(ep_history[:, 0])}\n",
    "                grads = sess.run(agent.gradients, feed_dict=feed_dict)\n",
    "                for idx, grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    feed_dict = dictionary = dict(zip(agent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(agent.update_batch, feed_dict=feed_dict)\n",
    "                    for ix, grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0\n",
    "\n",
    "                total_reward.append(running_reward)\n",
    "                total_lenght.append(j)\n",
    "                break\n",
    "\n",
    "\n",
    "                # Update our running tally of scores.\n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(total_reward[-100:]))\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
