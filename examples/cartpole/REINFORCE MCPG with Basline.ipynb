{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is addapted from Juliani's tutorial This is an implemenation of REINFORCE with a basline. \n",
    "See [Sutton's 2nd edition book Chapter 13.4](http://incompleteideas.net/book/bookdraft2017nov5.pdf) for a mathematical reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n",
      "/usr/local/lib/python2.7/dist-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discount function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add*gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_value(G, states, value):\n",
    "    b = sess.run(value.baseline, feed_dict={value.state_in: states})\n",
    "    return G - b.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\delta \\leftarrow G_t - \\hat{v}(S_t, \\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_gradient_zero(gradients):\n",
    "    for ix, grad in enumerate(gradients):\n",
    "        gradients[ix] = grad*0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, lr, s_size, a_size, h_size):\n",
    "        # These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "        self.state_in = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope(\"policy\"):\n",
    "            hidden = slim.fully_connected(self.state_in, h_size, biases_initializer=None, activation_fn=tf.nn.relu)\n",
    "            self.probability = slim.fully_connected(hidden, a_size, activation_fn=tf.nn.softmax, biases_initializer=None)\n",
    "        self.chosen_action = tf.argmax(self.probability, 1)\n",
    "\n",
    "        # The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "        # to compute the loss, and use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "\n",
    "        self.indexes = tf.range(0, tf.shape(self.probability)[0]) * tf.shape(self.probability)[1] + self.action_holder\n",
    "        # gets the probability which were associated with each action\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.probability, [-1]), self.indexes)\n",
    "\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs) * self.reward_holder)\n",
    "\n",
    "        tvars = tf.trainable_variables(\"policy\")\n",
    "        self.gradient_holders = []\n",
    "        with tf.variable_scope(\"policy\"):\n",
    "            for idx, var in enumerate(tvars):\n",
    "                placeholder = tf.placeholder(tf.float32, name=str(idx) + '_holder')\n",
    "                self.gradient_holders.append(placeholder)\n",
    "\n",
    "        self.gradients = tf.gradients(self.loss, tvars)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLine:\n",
    "\n",
    "    def __init__(self, lr, s_size, h_size):\n",
    "        self.state_in = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope(\"value\"):\n",
    "            hidden = slim.fully_connected(self.state_in, h_size, biases_initializer=None, activation_fn=tf.nn.relu)\n",
    "            self.baseline = slim.fully_connected(hidden, 1, activation_fn=tf.nn.relu, biases_initializer=None)\n",
    "\n",
    "        # The next lines establish the training procedure\n",
    "        self.delta = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "\n",
    "        self.loss = -tf.reduce_mean(self.baseline*self.delta)\n",
    "\n",
    "        tvars = tf.trainable_variables(\"value\")\n",
    "        self.gradient_holders = []\n",
    "        with tf.variable_scope(\"value\"):\n",
    "            for idx, var in enumerate(tvars):\n",
    "                placeholder = tf.placeholder(tf.float32, name=str(idx) + '_baseline_holder')\n",
    "                self.gradient_holders.append(placeholder)\n",
    "\n",
    "        self.gradients = tf.gradients(self.loss, tvars)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, tvars))        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "20.09\n",
      "29.92\n",
      "42.37\n",
      "60.29\n",
      "81.36\n",
      "120.1\n",
      "164.07\n",
      "170.54\n",
      "191.02\n",
      "194.31\n",
      "187.65\n",
      "197.0\n",
      "197.78\n",
      "195.26\n",
      "199.49\n",
      "199.54\n",
      "198.14\n",
      "190.25\n",
      "186.07\n",
      "196.33\n",
      "199.62\n",
      "196.15\n",
      "157.33\n",
      "152.0\n",
      "159.6\n",
      "169.71\n",
      "177.21\n",
      "163.65\n",
      "156.6\n",
      "164.14\n",
      "174.15\n",
      "181.76\n",
      "194.81\n",
      "197.77\n",
      "198.67\n",
      "200.0\n",
      "199.82\n",
      "199.46\n",
      "199.58\n",
      "199.93\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "199.43\n",
      "199.63\n",
      "199.82\n",
      "199.85\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "    tf.reset_default_graph()  # Clear the Tensorflow graph.\n",
    "\n",
    "    # setup the environment\n",
    "    env = gym.make('CartPole-v0')\n",
    "\n",
    "    agent = Agent(lr=1e-2, s_size=4, a_size=2, h_size=8)\n",
    "    value = BaseLine(lr=1e-2, s_size=4, h_size=8)\n",
    "\n",
    "    gamma = 0.99\n",
    "    total_episodes = 5000  # Set total number of episodes to train agent on.\n",
    "    max_ep = 999\n",
    "    update_frequency = 5  # how many MC samples of the gradient we gather before we make an update to the policy.\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        i = 0\n",
    "        total_reward = []\n",
    "        total_lenght = []\n",
    "\n",
    "        # Setting gradients of the policy to ZERO dc_dw1, dc_dw2\n",
    "        grad_policy_buffer = sess.run(tf.trainable_variables(\"policy\"))\n",
    "        grad_value_buffer = sess.run(tf.trainable_variables(\"value\"))\n",
    "\n",
    "        set_gradient_zero(grad_policy_buffer)\n",
    "        set_gradient_zero(grad_value_buffer)\n",
    "\n",
    "        while i < total_episodes:\n",
    "            s = env.reset()\n",
    "            running_reward = 0\n",
    "            ep_history = []\n",
    "            for j in range(max_ep):\n",
    "                # Probabilistically pick an action given our network outputs.\n",
    "                a_dist = sess.run(agent.probability, feed_dict={agent.state_in: [s]})\n",
    "                a = np.random.choice(a_dist[0], p=a_dist[0])\n",
    "                a = np.argmax(a_dist == a)\n",
    "\n",
    "                s1, r, done, _ = env.step(a)  # Get our reward for taking an action given a bandit.\n",
    "                ep_history.append([s, a, r, s1])\n",
    "                s = s1\n",
    "                running_reward += r\n",
    "\n",
    "                if done:\n",
    "                    # Update the network.\n",
    "                    ep_history = np.array(ep_history)\n",
    "\n",
    "                    S = np.vstack(ep_history[:, 0])\n",
    "\n",
    "                    A = ep_history[:, 1]\n",
    "\n",
    "                    # compute discounted rewards G\n",
    "                    G = discount_rewards(ep_history[:, 2], gamma)\n",
    "\n",
    "                    # compute delta\n",
    "                    delta = delta_value(G, S, value)\n",
    "\n",
    "                    # derivative of the value/baseline function with respect to its parameters\n",
    "                    value_grads = sess.run(value.gradients, feed_dict={value.delta: delta, value.state_in: S})\n",
    "                    # add gradient to the statistics\n",
    "                    for idx, v_grad in enumerate(value_grads):\n",
    "                        grad_value_buffer[idx] += v_grad\n",
    "\n",
    "                    # derivative of the policy function with respect to its parameters\n",
    "                    policy_grads = sess.run(agent.gradients, feed_dict={agent.reward_holder: delta, agent.action_holder: A, agent.state_in: S})\n",
    "                    # add gradient to the statistics\n",
    "                    for idx, p_grad in enumerate(policy_grads):\n",
    "                        grad_policy_buffer[idx] += p_grad\n",
    "\n",
    "                    # actually update the policy and value at specific intervals\n",
    "                    # after sufficient gradient statistics have been computed.\n",
    "                    if i % update_frequency == 0 and i != 0:\n",
    "                        # update value\n",
    "                        _ = sess.run(value.update_batch, feed_dict=dict(zip(value.gradient_holders, grad_value_buffer)))\n",
    "                        #  update policy\n",
    "                        _ = sess.run(agent.update_batch, feed_dict=dict(zip(agent.gradient_holders, grad_policy_buffer)))\n",
    "\n",
    "                        set_gradient_zero(grad_policy_buffer)\n",
    "                        set_gradient_zero(grad_value_buffer)\n",
    "\n",
    "                    total_reward.append(running_reward)\n",
    "                    total_lenght.append(j)\n",
    "                    break\n",
    "\n",
    "                    # Update our running tally of scores.\n",
    "            if i % 100 == 0:\n",
    "                print(np.mean(total_reward[-100:]))\n",
    "            i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
